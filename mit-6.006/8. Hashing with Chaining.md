# Hashing

https://www.youtube.com/watch?v=0M_kIqhwbFo

- fundamental - used a lot in Python
- Dictionary:
  - abstract data type (ADT)
  - a data structure
    - can insert items/delete items etc
  - maintain a set of items, each with a unique_key_
  - insert(item)
    - overwrites any existing key (python behaviour)
  - delete(item)
  - search(key) (not item)
    - returns an item with the given key
    - OR there isn't one: report error (KeyError in Python)
      - different to binary search trees: if we didn't find an item in that case, could find the next larger or smaller keys
      - just interested if it exists
- AVL trees? O(lg n) time
  - how to search faster than lg n time -> get down to O(1) (constant time) in most cases

Python:

- dict
- D[key]  -> search
- D[key] = val -> insert
- del D[key] -> delete
- item = (key, value)

How to actually implement?

Motivation:

- docdist (similarities between files -- finding common words, etc)
- database
- spellcheck: look it up in a dictionary
  - how to check misspelling and find the actual spelling?
  - = keep looking up in dictionary, tweaking a couple letters. Can afford to do this because so fast
- login systems
- compilers/interpreters:
  - translating variable names -> addresses, etc
- network router (need to go really fast)
- network server (which application is connected to this port)

more subtle applications (hashing):

- substring search
  - if editor is clever, will use hashing to search substrings
- string commonalities (DNA?)
- file/directory synchronisation
- cryptography: cryptographic hash functions

Let's do it:

# Simple Approach

= direct-access table

![image-20191130225236674](C:\Users\ab\Documents\SRes\LectureNotes\res\image-20191130225236674.png)

- store items in a big array - the index in the array is the key
- indexes are all restricted to non-negative integers
- if your keys all happen to be integers like that, could just use a giant array like this
  - lots of blank entries
- problems:
  - may be hard to associate something with a non-negative integer
  - gigantic memory hog
    - not always, though
    - if your set of possible keys is small, not bad
    - if you have a lotta keys, could be a lot though
      - 64 bit integer keys? Need HUGE amount of space
      - running cost of initialising that



Solution to keys that not integers:

prehashing (unfortunately, Python calls it hash though it isn't actually).

- maps keys to non-negative integers
- ideally: $$hash(x)=hash(y) \iff x=y$$
- theoretical answer to this:
  - in theory, keys are finite and discrete
    - anything on the computer can be represented as a string of bits
    - string of bits can be represented by an integer
    - = done
  - in reality, people don't always do this
    - in python, not quite so simple
    - hash(x) is  the prehash of x
    - maps every object to an integer
      - integer => returns the integer
      - string => some algorithm
        - some collisions do happen though
    - sadly, in python 'ideally' not always happen -- close though
    - can implement `__hash__` in python. Otherwise, Python will use space in memory -- good unless object moves around a lot in memory
- prehash should never change over time -- that would be very bad
  - this is why you can't use a list as a key (would potentially change over time)



Solution to too much space:

hashing

- reduce space of all keys and reduce them down into a small set of integers 
- ![image-20191130231832295](C:\Users\ab\Documents\SRes\LectureNotes\res\image-20191130231832295.png)
- want m to be approx n (n = # keys in dict)
  - = optimal
- problem: collisions



# Chaining

- a method of dealing with collisions
- idea:
  - if you have multiple things which hash to the same thing, just store those things as a list
- linked list of colliding elements in each slot of hashtable
- ![image-20191130232436359](C:\Users\ab\Documents\SRes\LectureNotes\res\image-20191130232436359.png)
- why would you expect it to be any good?
  - in worst case, all you have is a linked list => $\theta(n)$
  - usually, however, this won't happen though because ideally the distribution of hash function will be pretty spread out (randomisation to save the day woo)



Simple uniform Hashing

- an unrealistic assumption (almost false assumption), but gives you an idea
- each key is equally likely to be hashed to any slot of the table, independent of where other keys hashed
- each slot equally likely to be hashed too
- also independent
- can't really be true though (hashing is intrinsically a deterministic function kek)

analysis:

- expected length of chain for n keys, when you have m slots
  - = n/m
  - the chance of a key going to particular slot is 1/m, but all independent so 1/m + 1/m + .. + 1/m   n times
  - call n/m = $\alpha$ = load factor of the table
    - want to be 1, ideally
- running time
  - in worst case, compute hash function (assume takes constant time), then walk the linked list to the _end_ => O(1+len(chain)) => O(1+$\alpha$)



Hash Functions

- how do I construct h?

two of the following will be theoretically bad but practical, and the other is theoretically good

### 1) Division method

h(k) = k mod m

- especially bad if m has a common factor with k
- e.g: k is always even, and you want a table of size a power of 2
  - will only use half the table
- good if m is prime (factor issue)
  - and good if not close to power of 2 or power of 10
    - (because common in real world)

### 2) Multiplication method

$$
h(k) = [(a \times k)\ \text{mod}\ 2^w]>>(w-r)
$$

a some odd integer

- should not be very close to power of 2

where we are in a w bit machine

and m = $2^r$

expl: https://youtu.be/0M_kIqhwbFo?t=2656

- want the leftmost r bits of the rightmost w bits
- ![image-20191130235159188](C:\Users\ab\Documents\SRes\LectureNotes\res\image-20191130235159188.png)

- notice how in the centre, intuitively makes sense that a bunch of stuff is getting mixed together, and how in the edges, just copies pretty much





### 3) Universal Hashing

$$
h(k) = [(ak+b)\ \text{mod}\ p]\ \text{mod}\ m
$$

mod m is just the division method

a, b random numbers between 0 and p-1

where p is a prime number bigger than the size of the universe (can be found in polynomial time during initialisation)

turns out:

- worst case, k1 != k2
  - $p(h(k1)=h(k2)) = \frac{1}{m}$
- more details in 6.046